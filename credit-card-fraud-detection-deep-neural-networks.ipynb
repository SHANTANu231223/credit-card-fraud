{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit Card Fraud Detection::","metadata":{}},{"cell_type":"markdown","source":"# Description about dataset::","metadata":{}},{"cell_type":"markdown","source":"The datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \n\n\n### Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","metadata":{}},{"cell_type":"markdown","source":"# WORKFLOW :","metadata":{}},{"cell_type":"markdown","source":"1.Load Data\n\n2.Check Missing Values ( If Exist ; Fill each record with mean of its feature )\n\n3.Standardized the Input Variables. \n\n4.Split into 50% Training(Samples,Labels) , 30% Test(Samples,Labels) and 20% Validation Data(Samples,Labels).\n\n5.Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).\n\n6.Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)\n\n7.Train the Model with Epochs (100).\n\n8.If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .\n\n9.Prediction should be > 92%\n10.Evaluation Step\n11Prediction\n","metadata":{}},{"cell_type":"markdown","source":"# Task::","metadata":{}},{"cell_type":"markdown","source":"## Identify fraudulent credit card transactions.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport keras\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/creditcardfraud/creditcard.csv')","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9       V10       V11       V12       V13       V14  \\\n0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n\n        V15       V16       V17       V18       V19       V20       V21  \\\n0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n\n        V22       V23       V24       V25       V26       V27       V28  \\\n0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n\n   Amount  Class  \n0  149.62      0  \n1    2.69      0  \n2  378.66      0  \n3  123.50      0  \n4   69.99      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>V10</th>\n      <th>V11</th>\n      <th>V12</th>\n      <th>V13</th>\n      <th>V14</th>\n      <th>V15</th>\n      <th>V16</th>\n      <th>V17</th>\n      <th>V18</th>\n      <th>V19</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>0.090794</td>\n      <td>-0.551600</td>\n      <td>-0.617801</td>\n      <td>-0.991390</td>\n      <td>-0.311169</td>\n      <td>1.468177</td>\n      <td>-0.470401</td>\n      <td>0.207971</td>\n      <td>0.025791</td>\n      <td>0.403993</td>\n      <td>0.251412</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>-0.166974</td>\n      <td>1.612727</td>\n      <td>1.065235</td>\n      <td>0.489095</td>\n      <td>-0.143772</td>\n      <td>0.635558</td>\n      <td>0.463917</td>\n      <td>-0.114805</td>\n      <td>-0.183361</td>\n      <td>-0.145783</td>\n      <td>-0.069083</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>0.207643</td>\n      <td>0.624501</td>\n      <td>0.066084</td>\n      <td>0.717293</td>\n      <td>-0.165946</td>\n      <td>2.345865</td>\n      <td>-2.890083</td>\n      <td>1.109969</td>\n      <td>-0.121359</td>\n      <td>-2.261857</td>\n      <td>0.524980</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>-0.054952</td>\n      <td>-0.226487</td>\n      <td>0.178228</td>\n      <td>0.507757</td>\n      <td>-0.287924</td>\n      <td>-0.631418</td>\n      <td>-1.059647</td>\n      <td>-0.684093</td>\n      <td>1.965775</td>\n      <td>-1.232622</td>\n      <td>-0.208038</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>0.753074</td>\n      <td>-0.822843</td>\n      <td>0.538196</td>\n      <td>1.345852</td>\n      <td>-1.119670</td>\n      <td>0.175121</td>\n      <td>-0.451449</td>\n      <td>-0.237033</td>\n      <td>-0.038195</td>\n      <td>0.803487</td>\n      <td>0.408542</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(284807, 31)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Checking Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Time      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Splitting Data","metadata":{}},{"cell_type":"code","source":"data = df.iloc[:, :-1]\ntargets = df.iloc[:,-1]","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\nprint(targets.shape)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(284807, 30)\n(284807,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### data, data, targets, targets","metadata":{}},{"cell_type":"code","source":"train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=.3)","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data, validation_data, train_targets, validation_targets = train_test_split(train_data, train_targets, test_size=.2)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Standardizing Data","metadata":{}},{"cell_type":"code","source":"mean = np.mean(train_data)\nstd = np.std(train_data)\n\n\ntrain_data -= mean\ntrain_data /= std\n\nvalidation_data -= mean\nvalidation_data /= std\n\ntest_data -= mean\ntest_data /= std","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Model : input Layer (No. of features ), 3 hidden layers including 10,8,6 unit & Output Layer with activation function relu/tanh (check by experiment).","metadata":{}},{"cell_type":"code","source":"from keras import models, layers","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = models.Sequential()\n\nmodel.add(layers.Dense(10, input_shape=(train_data.shape[1],), activation='relu'))\n\nmodel.add(layers.Dense(8, activation='relu'))\n\nmodel.add(layers.Dense(6, activation='relu'))\n\nmodel.add(layers.Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Compilation Step","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_data, train_targets, epochs=100, batch_size=64)","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/100\n2493/2493 [==============================] - 4s 1ms/step - loss: 0.1421\nEpoch 2/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0063\nEpoch 3/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0058\nEpoch 4/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 5/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 6/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 7/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 8/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 9/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 10/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 11/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 12/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 13/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0049\nEpoch 14/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0052\nEpoch 15/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 16/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 17/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 18/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 19/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0049\nEpoch 20/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0052\nEpoch 21/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0058\nEpoch 22/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 23/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 24/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0041\nEpoch 25/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 26/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 27/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0033\nEpoch 28/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 29/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 30/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 31/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0054\nEpoch 32/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0039\nEpoch 33/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 34/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 35/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0037\nEpoch 36/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 37/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 38/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 39/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0041\nEpoch 40/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 41/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0052\nEpoch 42/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 43/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 44/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0046\nEpoch 45/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0049\nEpoch 46/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0039\nEpoch 47/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 48/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 49/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0039\nEpoch 50/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0038\nEpoch 51/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 52/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 53/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 54/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 55/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 56/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 57/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 58/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 59/100\n2493/2493 [==============================] - 4s 1ms/step - loss: 0.0042\nEpoch 60/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 61/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 62/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 63/100\n2493/2493 [==============================] - 4s 1ms/step - loss: 0.0043\nEpoch 64/100\n2493/2493 [==============================] - 4s 1ms/step - loss: 0.0049\nEpoch 65/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 66/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 67/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 68/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 69/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 70/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 71/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0042\nEpoch 72/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0038\nEpoch 73/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 74/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 75/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0046\nEpoch 76/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0046\nEpoch 77/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 78/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 79/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 80/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0048\nEpoch 81/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0036\nEpoch 82/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0039\nEpoch 83/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 84/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0049\nEpoch 85/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 86/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 87/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0040\nEpoch 88/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 89/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 90/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0057\nEpoch 91/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0043\nEpoch 92/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0047\nEpoch 93/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0044\nEpoch 94/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0041\nEpoch 95/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0049\nEpoch 96/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0050\nEpoch 97/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0045\nEpoch 98/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0046\nEpoch 99/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0057\nEpoch 100/100\n2493/2493 [==============================] - 3s 1ms/step - loss: 0.0054\n","output_type":"stream"}]},{"cell_type":"code","source":"val_predictions = model.predict(validation_data)","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"val_predictions","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([[3.1627471e-05],\n       [2.6116413e-06],\n       [2.3361859e-05],\n       ...,\n       [3.2347649e-05],\n       [2.4318080e-05],\n       [3.3504777e-05]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"preds = np.around(val_predictions)","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([[0.],\n       [0.],\n       [0.],\n       ...,\n       [0.],\n       [0.],\n       [0.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"print(classification_report(validation_targets, preds))","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     39797\n           1       0.87      0.82      0.84        76\n\n    accuracy                           1.00     39873\n   macro avg       0.94      0.91      0.92     39873\nweighted avg       1.00      1.00      1.00     39873\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Precision is 0.87.","metadata":{}},{"cell_type":"code","source":"test_preds = model.predict(test_data)","metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_preds.shape","metadata":{"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(85443, 1)"},"metadata":{}}]},{"cell_type":"code","source":"test_preds = test_preds.reshape(85443)","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_preds = np.round(test_preds)","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_targets, test_preds))","metadata":{"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85308\n           1       0.86      0.71      0.78       135\n\n    accuracy                           1.00     85443\n   macro avg       0.93      0.86      0.89     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### precision is 0.86","metadata":{}},{"cell_type":"code","source":"output_df = pd.DataFrame({'Actual': test_targets, 'Prediction': test_preds})","metadata":{"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"output_df.head()","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"        Actual  Prediction\n267580       0         0.0\n78917        0         0.0\n151007       1         1.0\n217694       0         0.0\n80201        0         0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>267580</th>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>78917</th>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>151007</th>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>217694</th>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>80201</th>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"output_df.shape","metadata":{"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(85443, 2)"},"metadata":{}}]},{"cell_type":"markdown","source":"# How many correct predictions?","metadata":{}},{"cell_type":"code","source":"filt = (output_df.loc[:, 'Actual'] == output_df.loc[:, 'Prediction'])\n\noutput_df[filt].shape","metadata":{"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(85388, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# print(f'Accuracy is {(output_df[filt].shape[0]/output_df.shape[0])*100}%')","metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Extra Work - Imbalanced Data\n\n-> finding ROC AUC Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(test_targets, test_preds)","metadata":{"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"0.8554617777152592"},"metadata":{}}]},{"cell_type":"markdown","source":"So, this is an excellent classifier according to this post [here](https://www.researchgate.net/post/What-is-the-value-of-the-area-under-the-roc-curve-AUC-to-conclude-that-a-classifier-is-excellent#:~:text=for%20Atomic%20Research-,What%20is%20the%20value%20of%20the%20area%20under%20the%20roc,1%20denotes%20an%20excellent%20classifier.)","metadata":{}},{"cell_type":"markdown","source":"-> Weighted Neural Networks","metadata":{}},{"cell_type":"code","source":"train_targets.value_counts()","metadata":{"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0    159210\n1       281\nName: Class, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"There are 281 1's (frauds) and 159210 0's (non-frauds)\n\n159210/281 = 566\n\n0's are 566 times more than 1's\n\nwe can assign custom weights to NN accordingly","metadata":{}},{"cell_type":"code","source":"weights = {\n    0:1,\n    1:566\n}","metadata":{"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = models.Sequential()\n\nmodel.add(layers.Dense(10, input_shape=(train_data.shape[1],), activation='relu'))\n\nmodel.add(layers.Dense(8, activation='relu'))\n\nmodel.add(layers.Dense(6, activation='relu'))\n\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy')\n\n# CHANGE\nhistory = model.fit(train_data, train_targets, epochs=100, validation_data=(validation_data, validation_targets), class_weight=weights)","metadata":{"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/100\n4985/4985 [==============================] - 23s 5ms/step - loss: 2.7055 - val_loss: 0.0046\nEpoch 2/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0444 - val_loss: 0.0055\nEpoch 3/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0363 - val_loss: 0.0052\nEpoch 4/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.1769 - val_loss: 0.0106\nEpoch 5/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7882 - val_loss: 0.0066\nEpoch 6/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9728 - val_loss: 0.0069\nEpoch 7/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7149 - val_loss: 0.0064\nEpoch 8/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4346 - val_loss: 0.0121\nEpoch 9/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.2697 - val_loss: 0.0058\nEpoch 10/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.1365 - val_loss: 0.0318\nEpoch 11/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.0917 - val_loss: 0.0061\nEpoch 12/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4713 - val_loss: 0.0063\nEpoch 13/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3699 - val_loss: 0.0144\nEpoch 14/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3807 - val_loss: 0.0117\nEpoch 15/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4267 - val_loss: 0.0072\nEpoch 16/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3511 - val_loss: 0.0066\nEpoch 17/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.2738 - val_loss: 0.0073\nEpoch 18/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3582 - val_loss: 0.0104\nEpoch 19/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3099 - val_loss: 0.0698\nEpoch 20/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.2810 - val_loss: 0.0062\nEpoch 21/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4647 - val_loss: 0.0086\nEpoch 22/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9610 - val_loss: 0.0093\nEpoch 23/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5394 - val_loss: 0.0480\nEpoch 24/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3589 - val_loss: 0.0095\nEpoch 25/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3297 - val_loss: 0.0445\nEpoch 26/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6865 - val_loss: 0.0091\nEpoch 27/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7062 - val_loss: 0.0115\nEpoch 28/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.2264 - val_loss: 0.0061\nEpoch 29/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3064 - val_loss: 0.0061\nEpoch 30/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3890 - val_loss: 0.0063\nEpoch 31/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3620 - val_loss: 0.0065\nEpoch 32/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7212 - val_loss: 0.0092\nEpoch 33/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6765 - val_loss: 0.0267\nEpoch 34/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4959 - val_loss: 4.5751\nEpoch 35/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4315 - val_loss: 0.0435\nEpoch 36/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5273 - val_loss: 0.0076\nEpoch 37/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.8731 - val_loss: 0.0074\nEpoch 38/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7996 - val_loss: 0.0084\nEpoch 39/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6608 - val_loss: 0.0080\nEpoch 40/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7041 - val_loss: 0.0953\nEpoch 41/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4945 - val_loss: 0.0085\nEpoch 42/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7519 - val_loss: 0.0107\nEpoch 43/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3780 - val_loss: 0.4795\nEpoch 44/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6835 - val_loss: 0.0083\nEpoch 45/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5719 - val_loss: 0.0080\nEpoch 46/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9043 - val_loss: 0.0094\nEpoch 47/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7190 - val_loss: 0.0079\nEpoch 48/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5490 - val_loss: 0.0087\nEpoch 49/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0587 - val_loss: 0.0281\nEpoch 50/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.2881 - val_loss: 0.0230\nEpoch 51/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7788 - val_loss: 0.0085\nEpoch 52/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6612 - val_loss: 2.7623\nEpoch 53/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4082 - val_loss: 0.0091\nEpoch 54/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5624 - val_loss: 0.0084\nEpoch 55/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4533 - val_loss: 0.0076\nEpoch 56/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5585 - val_loss: 0.0081\nEpoch 57/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5421 - val_loss: 0.0127\nEpoch 58/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3330 - val_loss: 0.0072\nEpoch 59/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4565 - val_loss: 0.4476\nEpoch 60/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4965 - val_loss: 0.1070\nEpoch 61/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3969 - val_loss: 0.4876\nEpoch 62/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.3088 - val_loss: 0.0068\nEpoch 63/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6061 - val_loss: 0.0075\nEpoch 64/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5979 - val_loss: 0.0083\nEpoch 65/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5524 - val_loss: 0.0074\nEpoch 66/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6486 - val_loss: 0.0095\nEpoch 67/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0353 - val_loss: 0.0075\nEpoch 68/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4104 - val_loss: 0.0094\nEpoch 69/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.1780 - val_loss: 0.0085\nEpoch 70/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.8826 - val_loss: 0.0118\nEpoch 71/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4302 - val_loss: 0.0071\nEpoch 72/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5368 - val_loss: 0.0086\nEpoch 73/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5108 - val_loss: 0.0070\nEpoch 74/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.4360 - val_loss: 0.0073\nEpoch 75/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6674 - val_loss: 0.0067\nEpoch 76/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.8390 - val_loss: 0.0064\nEpoch 77/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9278 - val_loss: 0.0070\nEpoch 78/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9337 - val_loss: 0.0086\nEpoch 79/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7896 - val_loss: 0.0073\nEpoch 80/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7404 - val_loss: 0.0118\nEpoch 81/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0018 - val_loss: 0.0074\nEpoch 82/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9609 - val_loss: 0.0088\nEpoch 83/100\n4985/4985 [==============================] - 9s 2ms/step - loss: 2.0663 - val_loss: 0.0122\nEpoch 84/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9153 - val_loss: 0.0103\nEpoch 85/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.2759 - val_loss: 0.0081\nEpoch 86/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.3283 - val_loss: 0.6205\nEpoch 87/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7406 - val_loss: 0.2621\nEpoch 88/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7697 - val_loss: 0.0126\nEpoch 89/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.3851 - val_loss: 0.0110\nEpoch 90/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.3164 - val_loss: 0.0092\nEpoch 91/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.0065 - val_loss: 0.0116\nEpoch 92/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.9268 - val_loss: 0.0059\nEpoch 93/100\n4985/4985 [==============================] - 9s 2ms/step - loss: 2.0979 - val_loss: 0.0314\nEpoch 94/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.5565 - val_loss: 0.0091\nEpoch 95/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.2605 - val_loss: 0.0081\nEpoch 96/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.7731 - val_loss: 0.0076\nEpoch 97/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.8694 - val_loss: 0.0092\nEpoch 98/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.3173 - val_loss: 0.0080\nEpoch 99/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 2.8846 - val_loss: 0.0109\nEpoch 100/100\n4985/4985 [==============================] - 8s 2ms/step - loss: 1.6203 - val_loss: 0.0196\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = model.predict(test_data)","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(test_targets, predictions)","metadata":{"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0.9407537220251151"},"metadata":{}}]},{"cell_type":"markdown","source":"# By using Cost Sensitive/ Weighted Neural Network, the ROC AUC Score is improved from 0.85 to 0.94 (outstanding classifier)","metadata":{}},{"cell_type":"code","source":"predictions = np.around(predictions)","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_targets, predictions))","metadata":{"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85308\n           1       0.76      0.73      0.74       135\n\n    accuracy                           1.00     85443\n   macro avg       0.88      0.87      0.87     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### precision is 0.76, which is less than before.","metadata":{}}]}